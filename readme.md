# ğŸš€ Corporate Knowledge Pilot

Asistente de IA conversacional para entornos corporativos, diseÃ±ado para responder preguntas basadas en una base de conocimiento interna y guiar al usuario a travÃ©s de un flujo de soluciÃ³n de problemas antes de crear tickets de soporte.

[![Estado de la ConstrucciÃ³n](https://github.com/rdiazm95/Corporate-Knowledge-Pilot/actions/workflows/ci.yaml/badge.svg)](https://github.com/rdiazm95/Corporate-Knowledge-Pilot/actions/workflows/ci.yaml)
[![Licencia: MIT](https://img.shields.io/badge/Licencia-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸŒŸ DemostraciÃ³n

AquÃ­ puedes ver al chatbot en acciÃ³n, respondiendo preguntas, pidiendo feedback y creando tickets de soporte solo cuando es necesario.

![Project Demo GIF](https://i.imgur.com/pNVPf3M.gif)

---

## âœ¨ CaracterÃ­sticas Principales

- **Arquitectura RAG (Retrieval-Augmented Generation):** El bot basa su conocimiento en un conjunto de documentos privados (PDFs, TXTs) para garantizar respuestas precisas y reducir alucinaciones.
- **Router de Intenciones:** Un LLM clasifica la intenciÃ³n del usuario (`pregunta general`, `reporte de problema`, `despedida`) para dirigir la conversaciÃ³n de forma inteligente.
- **Flujo de ConversaciÃ³n Guiado:** En lugar de crear un ticket directamente, el bot primero ofrece una soluciÃ³n de la base de conocimiento. Luego, pregunta explÃ­citamente al usuario si el problema se ha solucionado, implementando un **bucle de feedback** efectivo.
- **CreaciÃ³n de Tickets por AcciÃ³n:** Si la soluciÃ³n no es suficiente, el frontend ofrece al usuario la opciÃ³n de crear un ticket. Esta decisiÃ³n se comunica al backend mediante un mensaje de acciÃ³n especial (`ACTION_CREATE_TICKET`), demostrando un patrÃ³n de diseÃ±o robusto para agentes de IA.
- **Pila TecnolÃ³gica Local y Open-Source:** El sistema funciona 100% localmente usando Ollama con Llama 3.1 y modelos de embeddings de alto rendimiento, sin depender de APIs de pago.
- **Listo para Despliegue (Docker y Kubernetes):** El proyecto estÃ¡ completamente "dockerizado" y cuenta con manifiestos de Kubernetes para su orquestaciÃ³n, demostrando un flujo de trabajo listo para producciÃ³n.
- **CI/CD Automatizado:** Un workflow de GitHub Actions se encarga de construir y publicar automÃ¡ticamente las imÃ¡genes de Docker en Docker Hub cada vez que se actualiza el cÃ³digo.

---

## ğŸ—ï¸ Arquitectura del Sistema

La arquitectura consiste en un frontend que gestiona el estado de la conversaciÃ³n y un backend stateless que responde a cada pregunta a travÃ©s de un Ãºnico endpoint. La lÃ³gica de decisiÃ³n reside en el router de LangChain.

```mermaid
graph TD
    A[Usuario] --> B{Frontend React MUI};
    B --> C{Ingress Kubernetes};
    C -- ruta / --> B;
    C -- ruta /api/ask --> D{Backend FastAPI};
    D --> E[Router LangChain];
    E -- Intencion: pregunta_general --> F[RAG Chain];
    E -- Intencion: reporte_de_problema --> F;
    E -- Intencion: despedida --> G[Respuesta Directa];
    F --> H[(Vector Store - ChromaDB)];
    D -- ACTION_CREATE_TICKET --> I[Funcion de Tickets];
    I --> J[(DB - SQLite)];
    H --> K[Documentos PDF TXT];
``` 

## ğŸ’» Stack TecnolÃ³gico

| Ãrea | TecnologÃ­as |
|------|--------------|
| **Backend** | Python, FastAPI, LangChain, Ollama (Llama 3.1), Uvicorn |
| **IA & NLP** | RAG, Hugging Face Embeddings (`multilingual-e5-large`), ChromaDB |
| **Frontend** | React, TypeScript, Material-UI (MUI), Vite |
| **Base de Datos** | SQLite |
| **DevOps** | Docker, Docker Compose, Kubernetes, NGINX Ingress, GitHub Actions |

## ğŸ› ï¸ CÃ³mo Empezar

### ğŸ”§ Prerrequisitos

- ğŸ³ **Docker Desktop** instalado y funcionando (con Kubernetes activado).  
- ğŸ§  **Ollama** instalado y ejecutÃ¡ndose, con el modelo `llama3.1:8b` descargado:

```bash
ollama pull llama3.1:8b
```
### ğŸš€ OpciÃ³n 1: Docker (Recomendada)

Esta es la forma mÃ¡s sencilla de lanzar toda la aplicaciÃ³n.

#### 1ï¸âƒ£ Clona el repositorio

```bash
git clone https://github.com/tu-usuario/tu-repositorio.git
cd tu-repositorio
```
#### 2ï¸âƒ£ Clona el repositorio

```bash
docker-compose up --build
```
Accede a la interfaz ğŸ‘‰ http://localhost:5173

### ğŸ’» OpciÃ³n 2: Entorno de Desarrollo Local

#### 1ï¸âƒ£ ğŸ Backend

```bash
cd backend
python -m venv .venv
.venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload
```
---
#### 2ï¸âƒ£  ğŸ¨ Frontend
```bash
cd frontend
npm install
npm run dev
```
### â˜¸ï¸ OpciÃ³n 3: Despliegue en Kubernetes

#### 1ï¸âƒ£ ğŸ³ Publica tus ImÃ¡genes

AsegÃºrate de que tus imÃ¡genes **backend** y **frontend** estÃ¡n correctamente **etiquetadas y subidas a tu cuenta de Docker Hub**.

---

#### 2ï¸âƒ£ ğŸ§¾ Actualiza los Manifiestos

Edita el archivo:

```bash
kubernetes/manifests.yaml
```
Y reemplaza tu-usuario-docker por tu nombre de usuario en Docker Hub

---

 #### 3ï¸âƒ£ âœ… Aplica la ConfiguraciÃ³n 

```bash
kubectl apply -f kubernetes/manifests.yaml
```

---

#### 4ï¸âƒ£ Accede a la aplicaciÃ³n: Abre tu navegador en http://localhost.
(Interfaz en blanco)
![Project Demo GIF](https://i.imgur.com/XQrwFTG.gif)

---

### ğŸ—ºï¸ Hoja de Ruta (PrÃ³ximos Pasos)
Este proyecto seguirÃ¡ creciendo. Las siguientes fases planeadas son:

- ğŸ§±[ ] **Terraform:** Escribir cÃ³digo de Infraestructura como CÃ³digo (IaC) para provisionar un clÃºster de Kubernetes en la nube (ej. Azure     AKS).

- ğŸ“Š[x] **MonitorizaciÃ³n:** Integrar Prometheus y Grafana para monitorizar la salud y el rendimiento de la aplicaciÃ³n.

- ğŸ”’[ ] **Seguridad:** Implementar un sistema de autenticaciÃ³n (conceptual, con OAuth2) y mejorar los bucles de feedback de usuario para el reentrenamiento del modelo.

---

### ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la [Licencia MIT](https://opensource.org/license/MIT).

Â© 2025 â€” Desarrollado con â¤ï¸ por [rdiazm95](https://www.linkedin.com/in/rubendim/)
